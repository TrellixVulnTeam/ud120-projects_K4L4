{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何使用深度学习为照片自动生成文本描述\n",
    "\n",
    "## 定义\n",
    "对图像搜索和帮助视觉障碍者「查看」世界等应用而言，让图像带有文本描述是非常有价值的。使用人力标注显然不现实，而随着深度学习技术的发展，使用机器为图像自动生成准确的文本描述成为了可能。\n",
    "## 难点\n",
    "图像描述涉及到为给定图像（比如照片）生成人类可读的文本描述。这个问题对人类而言非常简单，但对机器来说却非常困难，因为它既涉及到理解图像的内容，还涉及到将理解到的内容翻译成自然语言。\n",
    "\n",
    "## 如何描述图像\n",
    "描述图像是指为图像（比如某个物体或场景的照片生成人类可读的文本描述）。这个问题有时成为自动图像标注或图像标注，这个问题对人类来说轻而易举，但对机器来说非常困难。要解决这个问题，既需要理解图像的内容，也需要将其中的含义用词语表达出来，并且所表达的词语必须以正确的方式串接起来才能被理解。这需要计算机视觉和自然语言处理结合起来，是广义的人工智能领域的一大难题。\n",
    "\n",
    "### 神经描述图像\n",
    "神经网络模型已经主导了自动描述生成领域；这主要是因为这种方法得到了当前最佳的结果。在端到端的神经网络模型之前，生成图像描述的两种主要方法是基于模板的方法和基于最近邻并修改已有描述的方法。\n",
    "用于描述的神经网络模型涉及到两个主要元素：\n",
    "\n",
    "* 特征提取\n",
    "* 语言模型\n",
    "\n",
    "#### 特征提取\n",
    "特征提取模型是一种神经网络。给定一张图像，它可以提取出显著的特征，通常用固定长度的向量表示。提取出的特征是该图像的内部表征，不是人类可以直接理解的东西。用作特征提取子模型的通常是深度卷积神经网络（CNN）。这种网络可以在图像描述数据集中的图像上直接训练。或者可以使用预训练的模型（比如用于图像分类的当前最佳的模型），或者也可以使用混合方法，即使用预训练的模型并根据实际问题进行微调。\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/0ee9d33d69285e3466eb5199fd8650e8.png)\n",
    "\n",
    "#### 语言模型\n",
    "一般而言，当一个序列已经给出了一些词时，语言模型可以预测该序列的下一个词的概率。\n",
    "对于图像描述，语言模型这种神经网络可以基于网络提取出的特征预测描述中的词序列并根据已经生成的词构建描述。\n",
    "常用的方法是使用循环神经网络作为语言模型，比如长短期记忆网络（LSTM）。每个输出时间步骤都会在序列中生成一个新词。\n",
    "然后每个生成的词都会使用一个词嵌入（比如 word2vec）进行编码，该编码会作为输入被传递给解码器以生成后续的词。\n",
    "对该模型的一种改进方法是为输出序列收集词在词汇库中的概率分布并搜索它以生成多个可能的描述。这些描述可以根据似然（likelihood）进行评分和排序。常见的方式是使用波束搜索（Beam Search）进行这种搜索。\n",
    "语言模型可以使用从图像数据集提取出的预计算的特征单独训练得到；也可以使用特征提取网络或某些组合方法来联合训练得到。\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/98647607cde8bd5e55be6d69f8f4f2bd.png)\n",
    "\n",
    "#### 编码器解码器架构\n",
    "构建子模型的一种常用方法是使用编码器-解码器架构，其中两个模型是联合训练的。\n",
    "\n",
    ">这种模型的基础是将图像编码成紧凑的表征的卷积神经网络，后面跟着一个循环神经网络来生成对应的句子。这种模型的训练目标是最大化给定图像的句子的似然。——《Show and Tell：一种神经图像描述生成器》，2015\n",
    "\n",
    "这种架构原本是为机器翻译开发的，其中输入的序列（比如法语）会被一个编码器网络编码成固定长度的向量。然后一个分立的解码器网络会读取这些编码并用另一种语言（比如英语）生成输出序列。\n",
    "\n",
    "除了能力出色外，这种方法的好处是可以在该问题上训练单个端到端模型。\n",
    "\n",
    "当将该方法用于图像描述时，编码器网络使用了深度卷积神经网络，解码器网络则是 LSTM 层的堆叠。\n",
    "\n",
    "> 在机器翻译中，「编码器」RNN 会读取源句子并将其转换成信息丰富的固定长度的向量表征，这种表征又会被用作「解码器」RNN 的初始隐藏状态，进而生成目标句子。我们在这里提出遵循这种优雅的方案，并使用深度卷积神经网络（CNN）替代编码器 RNN。——《Show and Tell：一种神经图像描述生成器》，2015\n",
    "\n",
    " ![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/f26e5b2fe0999db864680b439f747247.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras生成图像描述实战\n",
    "\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/1fd83e8552e584ecbdc3460eae410c6c.png)\n",
    "\n",
    "### 准备图像和描述数据集\n",
    "图像字幕生成可使用的优秀数据集有 Flickr8K 数据集。原因在于它逼真且相对较小，即使你的工作站使用的是 CPU 也可以下载它，并用于构建模型。\n",
    "作者对该数据集的描述如下：\n",
    "> 我们介绍了一种用于基于句子的图像描述和搜索的新型基准集合，包括 8000 张图像，每个图像有五个不同的字幕描述对突出实体和事件提供清晰描述。\n",
    "图像选自六个不同的 Flickr 组，往往不包含名人或有名的地点，而是手动选择多种场景和情形。\n",
    "\n",
    "该数据集的地址：\n",
    "* image数据：<http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_Dataset.zip>\n",
    "* 描述文本数据：<http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/Flickr8k_text.zip>\n",
    "\n",
    "\n",
    "下载数据集，并在当前工作文件夹里进行解压缩。你将得到两个目录：\n",
    "\n",
    "* Flicker8k_Dataset：包含 8092 张 JPEG 格式图像。\n",
    "* Flickr8k_text：包含大量不同来源的图像描述文件。\n",
    "该数据集包含一个预制训练数据集（6000 张图像）、开发数据集（1000 张图像）和测试数据集（1000 张图像）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 针对训练集中的图像进行特征提取\n",
    "\n",
    "由上文神经网络生成描述可得，第一步加载训练图像，我们需要用卷积神经网络对训练图像进行特征提取，生成固定长度的向量表示，该向量作为图像描述网络的编码器。Keras内置了已经训练好的cnn网络，我们不需要重新训练网络提取特征，直接使用训练好的vgg网络，去掉softmax分类层。因为我们对图像分类不感兴趣，我们感兴趣的是分类之前图像的内部表征，及图像的特征。\n",
    "    \n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/c5736385fbbe08b2bc3807a8d8e75216.png)\n",
    "\n",
    "Keras 还提供工具将加载图像改造成模型的偏好大小（如 3 通道 224 x 224 像素图像）。\n",
    "\n",
    "下面是 extract_features() 函数，即给出一个目录名，该函数将加载每个图像、为 VGG 准备图像数据，并从 VGG 模型中收集预测到的特征。图像特征是包含 4096 个元素的向量，该函数向图像特征返回一个图像标识符（identifier）词典。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "给一个目录名，该函数将加载每个图像，为VGG模型中收集预测到的特征，并从VGG模型中收集预测到的特征。\n",
    "图像特征是包含4096个元素的向量 ，该函数向图像特征返回一个图像标识符字典.\n",
    "\n",
    "\"\"\"\n",
    "def extract_features(directory):\n",
    "    # 加载VGG模型\n",
    "    model = VGG16()\n",
    "    \"\"\"弹出模型最后一层,最后一层是softmax的分类，这里我们只需要vgg全连接层\"\"\" \n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    step = 0\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # 将图像数据转化为numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        step += 1\n",
    "        if step % 50 == 0:\n",
    "            print('>%s' % name)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      ">106514190_bae200f463.jpg\n",
      ">1107471216_4336c9b328.jpg\n",
      ">116409198_0fe0c94f3b.jpg\n",
      ">124195430_d14028660f.jpg\n",
      ">1288909046_d2b2b62607.jpg\n",
      ">1332208215_fa824f6659.jpg\n",
      ">1363924449_487f0733df.jpg\n",
      ">1403414927_5f80281505.jpg\n",
      ">1432179046_8e3d75cf81.jpg\n",
      ">1463732807_0cdf4f22c7.jpg\n",
      ">1500853305_0150615ce9.jpg\n",
      ">1562478713_505ab6d924.jpg\n",
      ">166321294_4a5e68535f.jpg\n",
      ">1773928579_5664a810dc.jpg\n",
      ">1821238649_2fda79d6d7.jpg\n",
      ">1947351225_288d788983.jpg\n",
      ">2043427251_83b746da8e.jpg\n",
      ">2065875490_a46b58c12b.jpg\n",
      ">2085078076_b9db242d21.jpg\n",
      ">2098418613_85a0c9afea.jpg\n",
      ">2120411340_104eb610b1.jpg\n",
      ">2151056407_c9c09b0a02.jpg\n",
      ">2182488373_df73c7cc09.jpg\n",
      ">2198511848_311d8a8c2f.jpg\n",
      ">221973402_ecb1cd51f1.jpg\n",
      ">2244171992_a4beb04d8e.jpg\n",
      ">2259203920_6b93b721ce.jpg\n",
      ">2273591668_069dcb4641.jpg\n",
      ">228949397_9e63bfa775.jpg\n",
      ">2308108566_2cba6bca53.jpg\n",
      ">2328616978_fb21be2b87.jpg\n",
      ">2350400382_ced2b6c91e.jpg\n",
      ">2369248869_0266760c4a.jpg\n",
      ">2392625002_83a5a0978f.jpg\n",
      ">2410320522_d967f0b75c.jpg\n",
      ">241347441_d3dd9b129f.jpg\n",
      ">2425262733_afe0718276.jpg\n",
      ">244368383_e90b6b2f20.jpg\n",
      ">2455528149_6c3477fd33.jpg\n",
      ">2470519275_65725fd38d.jpg\n",
      ">2481367956_8577d2fa98.jpg\n",
      ">2503629305_055e9ec4b1.jpg\n",
      ">2521770311_3086ca90de.jpg\n",
      ">253762507_9c3356c2f6.jpg\n",
      ">2553550034_5901aa9d6c.jpg\n",
      ">2567035103_3511020c8f.jpg\n",
      ">2588625139_fdf6610218.jpg\n",
      ">260231029_966e2f1727.jpg\n",
      ">2619454551_c4bb726a85.jpg\n",
      ">2635908229_b9fc90d3fb.jpg\n",
      ">2653552905_4301449235.jpg\n",
      ">267015208_d80b3eb94d.jpg\n",
      ">2687529141_edee32649e.jpg\n",
      ">2701895972_8605c4e038.jpg\n",
      ">2715155329_1ed1756000.jpg\n",
      ">2735979477_eef7c680f9.jpg\n",
      ">2752919987_8bfca604ab.jpg\n",
      ">2769731772_18c44c18e2.jpg\n",
      ">2789937754_5d1fa62e95.jpg\n",
      ">2814028429_561a215259.jpg\n",
      ">2832453252_a06f7826a8.jpg\n",
      ">2846843520_b0e6211478.jpg\n",
      ">2860667542_95abec3380.jpg\n",
      ">2872806249_00bea3c4e7.jpg\n",
      ">2884252132_5d8e776893.jpg\n",
      ">2894850774_2d530040a1.jpg\n",
      ">2909875716_25c8652614.jpg\n",
      ">2924483864_cfdb900a13.jpg\n",
      ">2936590102_25036069a6.jpg\n",
      ">2950393735_9969c4ec59.jpg\n",
      ">2971431335_e192613db4.jpg\n",
      ">2987195421_e830c59fb6.jpg\n",
      ">3002448718_a478c64fb4.jpg\n",
      ">3016178284_ec50a09e8c.jpg\n",
      ">3029463004_c2d2c8f404.jpg\n",
      ">3042380610_c5ea61eef8.jpg\n",
      ">3052038928_9f53aa2084.jpg\n",
      ">3070011270_390e597783.jpg\n",
      ">3084034954_fe5737197d.jpg\n",
      ">3094278545_febac56382.jpg\n",
      ">3106791484_13e18c33d8.jpg\n",
      ">3117336911_a729f42869.jpg\n",
      ">3128856481_86e5df4160.jpg\n",
      ">3143155555_32b6d24f34.jpg\n",
      ">3154641421_d1b9b8c24c.jpg\n",
      ">316577571_27a0e0253e.jpg\n",
      ">3175434849_859f09fe07.jpg\n",
      ">3185695861_86152b2755.jpg\n",
      ">3198237818_cb5eb302f0.jpg\n",
      ">3208999896_dab42dc40b.jpg\n",
      ">3217187564_0ffd89dec1.jpg\n",
      ">3225310099_d8e419ba56.jpg\n",
      ">3239480519_22540b5016.jpg\n",
      ">3248220732_0f173fc197.jpg\n",
      ">3258391809_38fc6211f7.jpg\n",
      ">3264350290_f50494e835.jpg\n",
      ">327415627_6313d32a64.jpg\n",
      ">3286045254_696c6b15bd.jpg\n",
      ">3296150666_aae2f64348.jpg\n",
      ">3309578722_1765d7d1af.jpg\n",
      ">3322443827_a04a94bb91.jpg\n",
      ">3331190056_09f4ca9fd2.jpg\n",
      ">3339751521_7a8768be27.jpg\n",
      ">3349308309_92cff519f3.jpg\n",
      ">3356700488_183566145b.jpg\n",
      ">3366571152_20afb88ac1.jpg\n",
      ">3376809186_4e26d880b7.jpg\n",
      ">3393035454_2d2370ffd4.jpg\n",
      ">3401902253_cd27e6d0fe.jpg\n",
      ">3414734842_beb543f400.jpg\n",
      ">3422979565_e08cd77bfe.jpg\n",
      ">3429641260_2f035c1813.jpg\n",
      ">3438858409_136345fa07.jpg\n",
      ">3449114979_6cdc3e8da8.jpg\n",
      ">3458215674_2aa5e64643.jpg\n",
      ">3468346269_9d162aacfe.jpg\n",
      ">3477715432_79d82487bb.jpg\n",
      ">3487131146_9d3aca387a.jpg\n",
      ">3498423815_5b8fc097f4.jpg\n",
      ">3508882611_3947c0dbf5.jpg\n",
      ">3519155763_045a6a55e2.jpg\n",
      ">3528902357_be2357a906.jpg\n",
      ">3537201804_ce07aff237.jpg\n",
      ">354642192_3b7666a2dd.jpg\n",
      ">3556571710_19cee6f5bd.jpg\n",
      ">3564436847_57825db87d.jpg\n",
      ">3576741633_671340544c.jpg\n",
      ">358607894_5abb1250d3.jpg\n",
      ">3596131692_91b8a05606.jpg\n",
      ">3607405494_0df89110a6.jpg\n",
      ">3616771728_2c16bf8d85.jpg\n",
      ">3628698119_5566769777.jpg\n",
      ">3638688673_176f99d7fd.jpg\n",
      ">3647826834_dc63e21bd0.jpg\n",
      ">3658733605_fbcf570843.jpg\n",
      ">3671935691_57bdd0e778.jpg\n",
      ">3681324243_b69fa90842.jpg\n",
      ">3692836015_d11180727b.jpg\n",
      ">3705430840_e108de78bf.jpg\n",
      ">3717809376_f97611ab84.jpg\n",
      ">3730011701_5352e02286.jpg\n",
      ">381052465_722e00807b.jpg\n",
      ">397547349_1fd14b95af.jpg\n",
      ">412101267_7257e6d8c0.jpg\n",
      ">427557693_1108566fd2.jpg\n",
      ">441398149_297146e38d.jpg\n",
      ">456512643_0aac2fa9ce.jpg\n",
      ">470887781_faae5dae83.jpg\n",
      ">482907079_22085ada04.jpg\n",
      ">494792770_2c5f767ac0.jpg\n",
      ">506367606_7cca2bba9b.jpg\n",
      ">517094985_4b9e926936.jpg\n",
      ">531152619_6db02a7ed9.jpg\n",
      ">539751252_2bd88c456b.jpg\n",
      ">562928217_21f967a807.jpg\n",
      ">619169586_0a13ee7c21.jpg\n",
      ">697490420_67d8d2a859.jpg\n",
      ">758921886_55a351dd67.jpg\n",
      ">824782868_a8f532f3a6.jpg\n",
      ">887108308_2da97f15ef.jpg\n",
      ">95734035_84732a92c1.jpg\n",
      "Extracted Features: 8091\n"
     ]
    }
   ],
   "source": [
    "# extract features from all images\n",
    "import os\n",
    "directory = os.getcwd() + '/../data/Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备文本数据\n",
    "\n",
    "**文本数据预处理步骤:**\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/6989091d971bbbb62063fdb30f18f3ce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # remove punctuation from each token\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] =  ' '.join(desc)\n",
    "\n",
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "打印出加载图像描述的数量（8092）和干净词汇量的规模(8763个单词)，最后把干净的单词写入descriptions.txt\n",
    "\"\"\"    \n",
    "\n",
    "filename = os.getcwd() + '/../data/Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开发深度学习模型\n",
    "\n",
    "我们将定义深度学习模型，在训练数据集上进行拟合。\n",
    "\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/47808f818b3ab690365631d9d40f58a5.png)\n",
    "\n",
    "#### 加载数据\n",
    "\n",
    "必须加载准备好的图像和文本数据来拟合模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "  加载数据，我们必须加载准备好的图像和文本数据来拟合模型。我们将在训练数据集中的所有图像和描述上训练数据，训练过程中，我们将在开发数据集上监控模型\n",
    "  性能，使用该模型能确定什么时候保存模型至文件。\n",
    "  训练和开发数据集已经预制好，并分别保存在 Flickr_8k.trainImages.txt 和 Flickr_8k.devImages.txt 文件中，\n",
    "  二者均包含图像文件名列表。从这些文件名中，我们可以提取图像标识符，并使用它们为每个集过滤图像和描述。\n",
    "\"\"\"\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "'''\n",
    "该函数从给定标识符集的 descriptions.txt 中加载干净的文本描述，并向文本描述列表返回标识符词典。\n",
    "'''\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "'''\n",
    "下面定义了 load_photo_features() 函数，该函数加载了整个图像描述集，然后返回给定图像标识符集你感兴趣的子集。\n",
    "\n",
    "'''\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "\n",
    "'''\n",
    "运行该示例首先在测试数据集中加载 6000 张图像标识符。这些特征之后将用于加载干净描述文本和预计算的图像特征。\n",
    "\n",
    "'''\n",
    "# load training dataset (6K)\n",
    "filename = os.getcwd() + '/../data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras单词数据预处理\n",
    "keras中文文档：http://keras-cn.readthedocs.io/en/latest/preprocessing/text/\n",
    "\n",
    "#### 简介\n",
    "在进行自然语言处理之前，需要对文本进行处理。 \n",
    "本文介绍keras提供的预处理包keras.preproceing下的text与序列处理模块sequence模块\n",
    "\n",
    "#### text模块提供的方法\n",
    "* text_to_word_sequence(text,fileter) 可以简单理解此函数功能类str.split\n",
    "* one_hot(text,vocab_size) 基于hash函数(桶大小为vocab_size)，将一行文本转换向量表示（把单词数字化，vocab_size=5表示所有单词全都数字化在5以内）\n",
    "#### text.Tokenizer类\n",
    "这个类用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示。 \n",
    "init(num_words) 构造函数，传入词典的最大值\n",
    "\n",
    "##### 成员函数\n",
    "fit_on_text(texts) 使用一系列文档来生成token词典，texts为list类，每个元素为一个文档。\n",
    "texts_to_sequences(texts) 将多个文档转换为word下标的向量形式,shape为[len(texts)，len(text)] -- (文档数，每条文档的长度)\n",
    "texts_to_matrix(texts) 将多个文档转换为矩阵表示,shape为[len(texts),num_words]\n",
    "##### 成员变量\n",
    "document_count 处理的文档数量\n",
    "word_index 一个dict，保存所有word对应的编号id，从1开始\n",
    "word_counts 一个dict，保存每个word在所有文档中出现的次数\n",
    "word_docs 一个dict，保存每个word出现的文档的数量\n",
    "index_docs 一个dict，保存word的id出现的文档的数量\n",
    "##### 示例\n",
    "\n",
    "<p><code>\n",
    "    import keras.preprocessing.text as T\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "text1='some thing to eat'\n",
    "text2='some thing to drink'\n",
    "texts=[text1,text2]\n",
    "\n",
    "print T.text_to_word_sequence(text1)  #以空格区分，中文也不例外 ['some', 'thing', 'to', 'eat']\n",
    "print T.one_hot(text1,10)  #[7, 9, 3, 4] -- （10表示数字化向量为10以内的数字）\n",
    "print T.one_hot(text2,10)  #[7, 9, 3, 1]\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None) #num_words:None或整数,处理的最大单词数量。少于此数的单词丢掉\n",
    "tokenizer.fit_on_texts(texts)\n",
    "print( tokenizer.word_counts) #[('some', 2), ('thing', 2), ('to', 2), ('eat', 1), ('drink', 1)]\n",
    "print( tokenizer.word_index) #{'some': 1, 'thing': 2,'to': 3 ','eat': 4, drink': 5}\n",
    "print( tokenizer.word_docs) #{'some': 2, 'thing': 2, 'to': 2, 'drink': 1,  'eat': 1}\n",
    "print( tokenizer.index_docs) #{1: 2, 2: 2, 3: 2, 4: 1, 5: 1}\n",
    "\n",
    "####### num_words=多少会影响下面的结果，行数=num_words\n",
    "print( tokenizer.texts_to_sequences(texts)) #得到词索引[[1, 2, 3, 4], [1, 2, 3, 5]]\n",
    "print( tokenizer.texts_to_matrix(texts))  # 矩阵化=one_hot\n",
    "[[ 0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
    " [ 0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  0.,  0.]]\n",
    "\n",
    "\n",
    "'''将新闻文档处理成单词索引序列，单词与序号之间的对应关系靠单词的索引表word_index来记录'''\n",
    "#######例-------------------------------------------------------------------------\n",
    "\n",
    "tokenizer = Tokenizer(num_words=None) # 分词MAX_NB_WORDS\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "sequences = tokenizer.texts_to_sequences(all_texts) #受num_words影响\n",
    "word_index = tokenizer.word_index # 词_索引\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)  #将长度不足 100 的新闻用 0 填充（在前端填充）\n",
    "labels = to_categorical(np.asarray(all_labels)) #最后将标签处理成 one-hot 向量，比如 6 变成了 [0,0,0,0,0,0,1,0,0,0,0,0,0]，\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "####### Shape of data tensor: (81, 1000)  -- 81条数据\n",
    "####### Shape of label tensor: (81, 14)\n",
    "</p></code>    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7579\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "描述文本在作为输入馈送至模型或与模型预测进行对比之前需要先编码成数值。\n",
    "编码数据的第一步是创建单词到唯一整数值之间的持续映射。Keras 提供 Tokenizer class，可根据加载的描述数据学习该映射。\n",
    "\n",
    "下面定义了用于将描述词典转换成字符串列表的 to_lines() 函数，和对加载图像描述文本拟合 Tokenizer 的 create_tokenizer() 函数。\n",
    "\n",
    "'''\n",
    "# convert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# 这个类用来对文本中的词进行统计计数，生成文档词典，以支持基于词典位序生成文本的向量表示\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们现在对文本进行编码。\n",
    "\n",
    "每个描述将被分割成单词。我们向该模型提供一个单词和图像，然后模型生成下一个单词。描述的前两个单词和图像将作为模型输入以生成下一个单词，这就是该模型的训练方式。\n",
    "\n",
    "例如，输入序列「a little girl running in field」将被分割成 6 个输入-输出对来训练该模型：\n",
    "<pre><code>\n",
    "X1,\t\tX2 (text sequence), \t\t\t\t\t\t   y (word)\n",
    "photo\tstartseq, \t\t\t\t\t\t\t\t\t   little\n",
    "photo\tstartseq, little,\t\t\t\t\t\t\t   girl\n",
    "photo\tstartseq, little, girl, \t\t\t\t\t   running\n",
    "photo\tstartseq, little, girl, running, \t\t\t   in\n",
    "photo\tstartseq, little, girl, running, in, \t\t   field\n",
    "photo\tstartseq, little, girl, running, in, field,    endseq\n",
    "</pre></code>\n",
    "\n",
    "下面是 create_sequences() 函数，给出 tokenizer、最大序列长度和所有描述和图像的词典，该函数将这些数据转换成输入-输出对来训练模型。该模型有两个输入数组：一个用于图像特征，一个用于编码文本。模型输出是文本序列中编码的下一个单词。\n",
    "输入文本被编码为整数，被馈送至词嵌入层。图像特征将被直接馈送至模型的另一部分。\n",
    "该模型输出的预测是所有单词在词汇表中的概率分布。因此，输出数据是每个单词的 one-hot 编码，它表示一种理想化的概率分布，即除了实际词位置之外所有词位置的值都为 0，实际词位置的值为 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def create_sequences(tokenizer, max_length, descriptions, photos):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "'''\n",
    "我们需要计算最长描述中单词的最大数量。下面是一个有帮助的函数 max_length()。\n",
    "'''\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keras Embedding层\n",
    "嵌入层Embedding层将正整数（下标）转换为具有固定大小的向量，如[[4],[20]]->[[0.25,0.1],[0.6,-0.2]]\n",
    "\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/0fe93c90a11d261bb9a87aa965e05a94.png)\n",
    "\n",
    "上图的流程是把文章的单词使用词向量来表示。 \n",
    "(1)提取文章所有的单词，把其按其出现的次数降许(这里只取前50000个)，比如单词‘network’出现的次数最多，编号ID为0，依次类推…\n",
    "\n",
    "(2)每个编号ID都可以使用50000维的二进制(one-hot)表示\n",
    "\n",
    "(3)最后，我们会生产一个矩阵M，行大小为词的个数50000，列大小为词向量的维度(通常取128或300)，比如矩阵的第一行就是编号ID=0，即network对应的词向量。\n",
    "\n",
    "那这个矩阵M怎么获得呢？在Skip-Gram 模型中，我们会随机初始化它，然后使用神经网络来训练这个权重矩阵 \n",
    "\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/ef183e8d29f371a6b5fea1293933aa12.png)\n",
    "那我们的输入数据和标签是什么？如下图，输入数据就是中间的哪个蓝色的词对应的one-hot编码，标签就是它附近词的one-hot编码(这里windown_size=2,左右各取2个) \n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/c8ed067ecda4e45a15e7c35657c95514.png)\n",
    "就上述的Word2Vec中的demo而言，它的单词表大小为1000，词向量的维度为300，所以Embedding的参数 input_dim=10000，output_dim=300\n",
    "\n",
    "回到最初的问题：嵌入层将正整数（下标）转换为具有固定大小的向量，如[[4],[20]]->[[0.25,0.1],[0.6,-0.2]]\n",
    "\n",
    "举个栗子：假如单词表的大小为1000，词向量维度为2，经单词频数统计后，tom对应的id=4，而jerry对应的id=20，经上述的转换后，我们会得到一个M1000×2的矩阵，而tom对应的是该矩阵的第4行，取出该行的数据就是[0.25,0.1]\n",
    "\n",
    "如果输入数据不需要词的语义特征语义，简单使用Embedding层就可以得到一个对应的词向量矩阵，但如果需要语义特征，我们大可把以及训练好的词向量权重直接扔到Embedding层中即可，具体看参考keras提供的栗子:在Keras模型中使用预训练的词向量\n",
    "<https://github.com/MoyanZitto/keras-cn/blob/master/docs/legacy/blog/word_embedding.md>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "![1.jpg](https://image.jiqizhixin.com/uploads/wangeditor/d5fd5480-0b2c-462a-b71d-cc15d1a56f86/80570Schematic-of-the-Merge-Model-For-Image-Captioning.png)\n",
    "\n",
    "* 图像特征提取器：这是一个在 ImageNet 数据集上预训练的 16 层 VGG 模型。我们已经使用 VGG 模型（没有输出层）对图像进行预处理，并将使用该模型预测的提取特征作为输入。\n",
    "* 序列处理器：合适一个词嵌入层，用于处理文本输入，后面是长短期记忆（LSTM）循环神经网络层。\n",
    "* 解码器：特征提取器和序列处理器输出一个固定长度向量。这些向量由密集层（Dense layer）融合和处理，来进行最终预测。\n",
    "图像特征提取器模型的输入图像特征是维度为 4096 的向量，这些向量经过全连接层处理并生成图像的 256 元素表征。\n",
    "序列处理器模型期望馈送至嵌入层的预定义长度（34 个单词）输入序列使用掩码来忽略 padded 值。之后是具备 256 个循环单元的 LSTM 层。\n",
    "两个输入模型均输出 256 元素的向量。此外，输入模型以 50% 的 dropout 率使用正则化，旨在减少训练数据集的过拟合情况，因为该模型配置学习非常快。\n",
    "解码器模型使用额外的操作融合来自两个输入模型的向量。然后将其馈送至 256 个神经元的密集层，然后输送至最终输出密集层，从而在所有输出词汇上对序列中的下一个单词进行 softmax 预测。\n",
    "\n",
    "\n",
    "**该模型的网络结构为**\n",
    "\n",
    "![2.png](https://image.jiqizhixin.com/uploads/wangeditor/d5fd5480-0b2c-462a-b71d-cc15d1a56f86/87019Plot-of-the-Caption-Generation-Deep-Learning-Model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "现在我们已经了解如何定义模型了，那么接下来我们要在训练数据集上拟合模型。\n",
    "该模型学习速度快，很快就会对训练数据集产生过拟合。因此，我们需要在留出的开发数据集上监控训练模型的泛化情况。如果模型在开发数据集上的技能在每个 epoch 结束时有所提升，则我们将整个模型保存至文件。\n",
    "在运行结束时，我们能够使用训练数据集上具备最优技能的模型作为最终模型。\n",
    "通过在 Keras 中定义 ModelCheckpoint，使之监控验证数据集上的最小损失，我们可以实现以上目的。然后将该模型保存至文件名中包含训练损失和验证损失的文件中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-05ae17f8c618>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Description Length: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m# prepare sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mX1train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_descriptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# dev dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-2fd83971a7ce>\u001b[0m in \u001b[0;36mcreate_sequences\u001b[1;34m(tokenizer, max_length, descriptions, photos)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[0mX2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                 \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_seq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m '''\n\u001b[0;32m     23\u001b[0m \u001b[0m我们需要计算最长描述中单词的最大数量\u001b[0m\u001b[0;31m。\u001b[0m\u001b[0m下面是一个有帮助的函数\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m。\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "我们还需要 fit() 中的 validation_data 参数指定开发数据集。\n",
    "我们仅拟合模型 20 epoch，给出一定量的训练数据，在一般硬件上每个 epoch 可能需要 30 分钟。\n",
    "'''\n",
    "# train dataset\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = os.getcwd() + '/../data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\n",
    "\n",
    "# dev dataset\n",
    "\n",
    "# load test set\n",
    "filename = os.getcwd() + '/../data/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)\n",
    "\n",
    "# fit model\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# fit model\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "模型拟合之后，我们可以在留出的测试数据集上评估它的预测技能。\n",
    "\n",
    "使模型对测试数据集中的所有图像生成描述，使用标准代价函数评估预测，从而评估模型。\n",
    "\n",
    "首先，我们需要使用训练模型对图像生成描述。输入开始描述的标记 『startseq『，生成一个单词，然后递归地用生成单词作为输入启用模型直到序列标记到 『endseq『或达到最大描述长度。\n",
    "\n",
    "下面的 generate_desc() 函数实现该行为，并基于给定训练模型和作为输入的准备图像生成文本描述。它启用 word_for_id() 函数以映射整数预测至单词。\n",
    "\n",
    "'''\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "'''\n",
    "我们将为测试数据集和训练数据集中的所有图像生成预测。\n",
    "下面的 evaluate_model() 基于给定图像描述数据集和图像特征评估训练模型。收集实际和预测描述，使用语料库 BLEU 值对它们进行评估。语料库 BLEU 值总结了生成文本和期望文本之间的相似度。\n",
    "'''\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成新的图像字幕\n",
    "\n",
    "现在我们了解了如何开发和评估字幕生成模型，那么我们如何使用它呢？\n",
    "\n",
    "我们需要模型文件中全新的图像，还需要 Tokenizer 用于对模型生成单词进行编码，生成序列和定义模型时使用的输入序列最大长度。\n",
    "\n",
    "我们可以对最大序列长度进行硬编码。文本编码后，我们就可以创建 tokenizer，并将其保存至文件，这样我们可以在需要的时候快速加载，无需整个 Flickr8K 数据集。另一个方法是使用我们自己的词汇文件，在训练过程中将其映射到取整函数。\n",
    "\n",
    "我们可以按照之前的方式创建 Tokenizer，并将其保存为 pickle 文件 tokenizer.pkl。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# load training dataset (6K)\n",
    "filename = os.getcwd() + '/../data/Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以在需要的时候加载 tokenizer，无需加载整个标注训练数据集。下面，我们来为一个新图像生成描述，下面这张图是我从 Flickr 中随机选的一张图像。\n",
    "![image.png](http://ata2-img.cn-hangzhou.img-pub.aliyun-inc.com/bfcfbe37a06a0a61e0fb11b1e1d58e69.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "预测单幅图像的描述\n",
    "'''\n",
    "from pickle import load\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model.layers.pop()\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('model-ep005-loss3.533-val_loss3.834.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features('example.jpg')\n",
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "img = Image.open('example.jpg')\n",
    "\n",
    "plt.figure(\"Image\") # 图像窗口名称\n",
    "plt.imshow(img)\n",
    "plt.axis('on') # 关掉坐标轴为 off\n",
    "plt.title('image') # 图像题目\n",
    "plt.show()\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
